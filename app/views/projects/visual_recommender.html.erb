
<script>
    $( document ).ready(function() {
        var viewer = OpenSeadragon({
            id: "openseadragon-viewer",
            prefixUrl: "/assets/openseadragon-icon/",
            tileSources: "/assets/book_recommendation_skipgram.dzi",
            showNavigator: true
        });
        viewer.addHandler('open', function () {
            if (gon.zoom_center !== -1) {
                imagex=gon.zoom_center[0];
                imagey=gon.zoom_center[1];
                viewer.viewport.fitBounds(viewer.viewport.imageToViewportRectangle(imagex-225,imagey-300,600,800), true);
            }
        });

        $("input[id='books-search-txt']").autocomplete({
            source: '/projects/visual_recommender_book_search'
        });
    })
</script>

<div class="container">
  <h1> A Visual Recommender trained on Wikipedia data using Word2Vec skip-grams model</h1>
  <dl>
    <dt> Data </dt>
    <dd>
      Almost every Wikipedia page contain links to other entities.
      Relationships among entities could be derived from whether or not two entites share same links
      as well as the number of links shared.

      Wikimedia.org provide a complete data dump of all wiki pages which can be parsed and stored
      in the database.
    </dd>

    <dt> Model </dt>
    <dd>
      a simple co-occurrence matrix could be generated.
      finding the similarity between entities will simply be calculating the jaccard distance between two
      vectors.
      however, high dimensionality would involve large amount of calculation when we try to find relationships
      among all entities.

      Word2Vec skip grams model produce word embeddings by training a neural network for a specific task.
      the generated Embedding vectors use distributed representation to represent each entity, which greatly reduce
      the number of dimensions.

      For this project, the task would be to determine if two entities share common references.
    </dd>

    <dt> Implementation </dt>
    <a href="https://github.com/luliu31415926/visual_recommender/blob/master/.ipynb_checkpoints/Book_Recommendations-Skipgram-checkpoint.ipynb">source code is on Github</a>
    <dd>
      the Word2Vec model was trained using KERAS API in Python Jupyter notebook
      data was stored in postgreSQL database
      demo is hosted online with Ruby on Rails with openseadragon API

      using wikistats view count , I selected the top 500 wiki books to train as a demo.
      after reducing the dimension to 20, I mapped the embedding vectors to 2d using TSNE model for visualization

      the result is not perfect but reasonally well, you can play with it in the demo below.
      (the images are not the most accurate as they are retrieved from wiki data)
    </dd>
    <dt> Demo </dt>
    <%= form_tag "/projects/visual_recommender", method: "get" , class:"col-lg-4" do %>
        <label for="books-search-txt">Pick a book: </label>
        <%= text_field_tag 'books-search-txt' %>
        <%= submit_tag 'Go!', class:"btn btn-success"%>
    <% end %>
    <div class="nearest-neighbors col-lg-4">
      <% if @book %>
          <table>
            <caption>10 Most Similar Books:</caption>
            <% @neighbors.each do |n| %>
                <tr><td><%= n.title %></td></tr>
            <% end %>
          </table>
      <% end %>
    </div>
    <div id="openseadragon-viewer" class="col-lg-8" style="width: 600px; height: 800px;"></div>
  </dl>
</div>